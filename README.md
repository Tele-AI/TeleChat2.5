<div align="center">
<h1>
  TeleChat2.5
</h1>
</div>


<p align="center">
   ğŸ¦‰ <a href="https://github.com/Tele-AI/TeleChat2.5" target="_blank">github</a> â€¢ ğŸ¤— <a href="https://huggingface.co/Tele-AI" target="_blank">Hugging Face</a> â€¢ ğŸ¤– <a href="https://modelscope.cn/organization/TeleAI" target="_blank">ModelScope</a> â€¢ ğŸ¾ <a href="https://gitee.com/Tele-AI/TeleChat2.5" target="_blank">gitee</a> â€¢ ğŸ’¬ <a href="https://github.com/Tele-AI/Telechat/blob/master/images/wechat.jpg" target="_blank">WeChat</a>
</p>

# ç›®å½•

- [æ¨¡å‹ä»‹ç»](#æ¨¡å‹ä»‹ç»)
- [æ•ˆæœè¯„æµ‹](#æ•ˆæœè¯„æµ‹)
- [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
- [å›½äº§åŒ–é€‚é…](#å›½äº§åŒ–é€‚é…)
- [å£°æ˜ã€åè®®ã€å¼•ç”¨](#å£°æ˜åè®®å¼•ç”¨)

# æ¨¡å‹ä»‹ç»

**TeleChat2.5** æ˜¯ **TeleChat** ç³»åˆ—æ–°ç‰ˆé€šç”¨é—®ç­”æ¨¡å‹ï¼Œç”±ä¸­å›½ç”µä¿¡äººå·¥æ™ºèƒ½ç ”ç©¶é™¢ï¼ˆ**TeleAI**ï¼‰åŸºäºå›½äº§ç®—åŠ›ç ”å‘è®­ç»ƒï¼ŒåŒ…æ‹¬äº† **TeleChat2.5-35B** ä¸ **TeleChat2.5-115B**ã€‚TeleChat2.5 åŸºäºæœ€æ–°å¼ºåŒ–çš„ TeleBase2.5 ç³»åˆ—æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œåœ¨ç†ç§‘ã€é€šç”¨é—®ç­”ã€Function Callç­‰ä»»åŠ¡ä¸Šæœ‰æ˜¾è‘—çš„æ•ˆæœæå‡ã€‚TeleChat2.5 çš„å¾®è°ƒæ–¹æ³•å»¶ç»­äº† TeleChat2 ç³»åˆ—ï¼Œå…·ä½“è¯·å‚è€ƒ [TeleChat2](https://github.com/Tele-AI/TeleChat2)ã€‚

### è®­ç»ƒç­–ç•¥
#### æ•°æ®

- ä¸ºäº†æé«˜æ¨¡å‹è®­ç»ƒæ•°æ®çš„æ•°é‡å’Œè´¨é‡ï¼ŒTeleChat2.5 åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†å¤§é‡ç†ç§‘å­¦ç§‘å’Œç¼–ç¨‹é¢†åŸŸçš„åˆæˆæ•°æ®ã€‚åœ¨åˆæˆè¿‡ç¨‹ä¸­ï¼Œä¸ºäº†å‡å°‘é”™è¯¯ä¿¡æ¯çš„å¼•å…¥ï¼Œä¸»è¦ä»¥åŸºäºçŸ¥è¯†ç‚¹æˆ–çŸ¥è¯†ç‰‡æ®µçš„æ•™è‚²ç±»çŸ¥è¯†åˆæˆä¸ºä¸»ã€‚


#### åŸºç¡€æ¨¡å‹è®­ç»ƒ

- TeleChat2.5 é‡‡ç”¨äº†å¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥æå‡ç†ç§‘å’Œç¼–ç¨‹ç±»é«˜å¯†åº¦çŸ¥è¯†æ•°æ®çš„æ¯”ä¾‹ã€‚æ¯ä¸ªè®­ç»ƒé˜¶æ®µéƒ½ä½¿ç”¨æ¯”å‰ä¸€é˜¶æ®µè´¨é‡æ›´é«˜ã€éš¾åº¦æ›´å¤§çš„æ•°æ®ï¼Œä»¥å®ç°æŒç»­çš„æ¨¡å‹ä¼˜åŒ–ã€‚  

- åœ¨æœ€ç»ˆè®­ç»ƒé˜¶æ®µï¼Œä¸ºäº†å¹³è¡¡æ¨¡å‹åœ¨å„ä¸ªç»´åº¦çš„èƒ½åŠ›è¡¨ç°ï¼Œæˆ‘ä»¬é€‰å–äº†ä¸åŒè®­ç»ƒé˜¶æ®µæ•ˆæœè¾ƒä¼˜çš„å¤šä¸ªæ¨¡å‹ï¼Œå¹¶åŸºäºå„æ¨¡å‹çš„ç»¼åˆè¡¨ç°è¿›è¡Œå‚æ•°åŠ æƒèåˆï¼Œå…¶ä¸­æƒé‡åˆ†é…ä¸æ¨¡å‹æ€§èƒ½å‘ˆæ­£ç›¸å…³ã€‚

#### åè®­ç»ƒé˜¶æ®µ
æˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µä¼˜åŒ–çš„æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼š

- èåˆä¼˜åŒ–é˜¶æ®µï¼šæ•´åˆå¤æ‚æ¨ç†ä¸é€šç”¨é—®ç­”èƒ½åŠ›ï¼Œé’ˆå¯¹è¯­è¨€ç†è§£ã€æ•°ç†é€»è¾‘ç­‰è–„å¼±ä»»åŠ¡è¿›è¡Œè§£æ„é‡ç»„ã€‚é€šè¿‡é‡æ„ä»»åŠ¡æ¡†æ¶å¹¶èåˆå¤šç»´åº¦è§£é¢˜æ€è·¯ï¼Œç”Ÿæˆä¼˜åŒ–åçš„é€šç”¨ç­”æ¡ˆé›†ã€‚æ­¤é˜¶æ®µç­”æ¡ˆé•¿åº¦ä¼šé€‚åº¦å¢åŠ ï¼Œå¹¶åŸºäºä¼˜åŒ–æ•°æ®å®æ–½å¾®è°ƒè®­ç»ƒã€‚

- èƒ½åŠ›å¼ºåŒ–é˜¶æ®µï¼šé’ˆå¯¹æ•°ç†é€»è¾‘ä¸ç¼–ç¨‹ç±»ä»»åŠ¡ï¼Œé€šè¿‡æ³¨å…¥ç»“æ„åŒ–è§£é¢˜æ€è·¯ï¼Œç»“åˆåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶ï¼Œæ˜¾è‘—æå‡æ¨¡å‹å¯¹å¤æ‚ä»»åŠ¡çš„ç†è§£ä¸å¤„ç†èƒ½åŠ›ã€‚

- æ³›åŒ–æå‡é˜¶æ®µï¼šé¢å‘å®‰å…¨åˆè§„ã€æŒ‡ä»¤å“åº”ã€å‡½æ•°è°ƒç”¨ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆç­‰åä½™ç§ä»»åŠ¡ç±»å‹è¿›è¡Œç³»ç»Ÿæ€§å¼ºåŒ–å­¦ä¹ å¢å¼ºï¼Œå…¨é¢æå‡æ¨¡å‹çš„é€šç”¨ä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚

### æ¨¡å‹ä¸‹è½½
| æ¨¡å‹ç‰ˆæœ¬             | ä¸‹è½½é“¾æ¥                                                          | 
|------------------|--------------------------------------------------------------------|
| TeleChat2.5-35B  | [modelscope](https://modelscope.cn/models/TeleAI/TeleChat2.5-35B)  |
| TeleChat2.5-115B | [modelscope](https://modelscope.cn/models/TeleAI/TeleChat2.5-115B) |

# æ•ˆæœè¯„æµ‹
| æ¨¡å‹               | MATH-500 | AlignBench | BFCL(avg v1&v2) |
|------------------|----------|------------|-----------------|
| Qwen2.5-32B      | 82       | 7.39       | 81.11           |
| Qwen2.5-72B      | 82       | 7.62       | 79.15           |
| Qwen3-32Bï¼ˆé€šç”¨ï¼‰    | 83       | 8.23       | 81.84           |
| GPT-4o-1120      | 75       | 7.49       | 78.66           |
| TeleChat2-35B    | 65       | 6.97       | 75.32           |
| TeleChat2-115B   | 75       | 7.56       | 77.47           |
| TeleChat2.5-35B  | 85        | 7.73       | 78.28           |
| TeleChat2.5-115B | 87       | 7.93       | 83.39           |


# æ¨¡å‹æ¨ç†

TeleChat2.5 ç³»åˆ—æ¨¡å‹æ”¯æŒä½¿ç”¨ `transformers` åº“è¿›è¡Œæ¨ç†ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š


```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("TeleChat2.5/TeleChat2.5-35B", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "TeleChat2.5/TeleChat2.5-35B",
    trust_remote_code=True, 
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
prompt = "ç”ŸæŠ½å’Œé…±æ²¹çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

#### ModelScope
TeleChat2.5 ç³»åˆ—æ¨¡å‹æ”¯æŒä½¿ç”¨ ModelScope æ¨ç†ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
import os
import torch
from modelscope import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
tokenizer = AutoTokenizer.from_pretrained('TeleChat2.5/TeleChat2.5-35B', trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained('TeleChat2.5/TeleChat2.5-35B', trust_remote_code=True, device_map="auto",
                                                  torch_dtype=torch.bfloat16)
prompt = "ç”ŸæŠ½ä¸è€æŠ½çš„åŒºåˆ«ï¼Ÿ"
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```


### vLLM æ¨ç†

TeleChat2.5 æ”¯æŒä½¿ç”¨ [vLLM](https://github.com/vllm-project/vllm) è¿›è¡Œéƒ¨ç½²ä¸æ¨ç†åŠ é€Ÿï¼Œç¤ºä¾‹å¦‚ä¸‹:
##### ç¦»çº¿æ¨ç†
```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

tokenizer = AutoTokenizer.from_pretrained("TeleChat2.5/TeleChat2.5-35B", trust_remote_code=True)
sampling_params = SamplingParams(temperature=0.0, repetition_penalty=1.01, max_tokens=8192)
llm = LLM(model="TeleChat2.5/TeleChat2.5-35B", trust_remote_code=True, tensor_parallel_size=4, dtype="bfloat16")

prompt = "ç”ŸæŠ½å’Œé…±æ²¹çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

outputs = llm.generate([text], sampling_params)
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

##### OpenAI å…¼å®¹çš„ API æœåŠ¡
æ‚¨å¯ä»¥å€ŸåŠ© vLLMï¼Œæ„å»ºä¸€ä¸ªä¸ OpenAI API å…¼å®¹çš„ API æœåŠ¡ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ‰€ç¤ºè¿è¡Œå‘½ä»¤ï¼š
```
vllm serve TeleChat2.5/TeleChat2.5-35B \
    --trust-remote-code \
    --dtype bfloat16 \
    --disable-custom-all-reduce
```
ç„¶åï¼Œæ‚¨å¯ä»¥ä¸ TeleChat2.5 è¿›è¡Œå¯¹è¯ï¼š
```python
from openai import OpenAI
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(api_key=openai_api_key, base_url=openai_api_base)
chat_response = client.chat.completions.create(
    model="TeleChat2.5/TeleChat2.5-35B",
    messages=[
        {"role": "user", "content": "ç”ŸæŠ½å’Œé…±æ²¹çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"},
    ],
    temperature=0.0,
    max_tokens=8192,
    extra_body={
        "repetition_penalty": 1.01,
        "skip_special_tokens": False,
        "spaces_between_special_tokens": False,
    },
)
print("Chat response:", chat_response)
```

# å›½äº§åŒ–é€‚é…

TeleChat2.5ç³»åˆ—æ¨¡å‹å‡è¿›è¡Œäº†**å›½äº§åŒ–ç®—åŠ›é€‚é…**ï¼Œå…·ä½“ä¿¡æ¯å¯è§
1. <a href="https://modelers.cn/models/MindSpore-Lab/TeleChat2.5-35B" target="_blank">MindSpore-Lab/TeleChat2.5-35B</a>
2. <a href="https://modelers.cn/models/MindSpore-Lab/TeleChat2.5-115B" target="_blank">MindSpore-Lab/TeleChat2.5-115B</a>

# å£°æ˜ã€å¼•ç”¨

### å£°æ˜

æˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œä¸è¦ä½¿ç”¨ TeleChat2.5 ç³»åˆ—æ¨¡å‹åŠå…¶è¡ç”Ÿæ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°† TeleChat2.5 ç³»åˆ—æ¨¡å‹ç”¨äºæ²¡æœ‰å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰ä½¿ç”¨è€…éµå®ˆä¸Šè¿°åŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€å‘å±•åœ¨åˆæ³•åˆè§„çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚

æˆ‘ä»¬å·²ç»å°½æˆ‘ä»¬æ‰€èƒ½ï¼Œæ¥ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚æœç”±äºä½¿ç”¨ TeleChat2.5 ç³»åˆ—å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

### å¼•ç”¨

å¦‚éœ€å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·ä½¿ç”¨å¦‚ä¸‹ reference:

```
@misc{wang2025technicalreporttelechat2telechat25,
      title={Technical Report of TeleChat2, TeleChat2.5 and T1}, 
      author={Zihan Wang and Xinzhang Liu and Yitong Yao and Chao Wang and Yu Zhao and Zhihao Yang and Wenmin Deng and Kaipeng Jia and Jiaxin Peng and Yuyao Huang and Sishi Xiong and Zhuo Jiang and Kaidong Yu and Xiaohui Hu and Fubei Yao and Ruiyu Fang and Zhuoru Jiang and Ruiting Song and Qiyi Xie and Rui Xue and Xuewei He and Yanlei Xue and Zhu Yuan and Zhaoxi Zhang and Zilu Huang and Shiquan Wang and Xin Wang and Hanming Wu and Mingyuan Wang and Xufeng Zhan and Yuhan Sun and Zhaohu Xing and Yuhao Jiang and Bingkai Yang and Shuangyong Song and Yongxiang Li and Zhongjiang He and Xuelong Li},
      year={2025},
      eprint={2507.18013},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.18013}, 
}
```

